{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Labo 6\n",
        "\n",
        "Lucas Charbonnier & Rémi Ançay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training files :  7769\n",
            "testing files :  3019\n",
            "total files :  10788\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to\n",
            "[nltk_data]     C:\\Users\\lcsch\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import reuters\n",
        "import nltk\n",
        "\n",
        "nltk.download('reuters')\n",
        "\n",
        "print('training files : ', len([fid for fid in reuters.fileids() if fid[:5] == 'train']))\n",
        "print('testing files : ', len([fid for fid in reuters.fileids() if fid[:4] == 'test']))\n",
        "print('total files : ', len(reuters.fileids()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1\n",
        "\n",
        "Il y a 10788 documents au total, dont 7769 d'entraînement et 3019 de test. Nous rechercherons les paramètres optimaux sur les documents de test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2\n",
        "Transformation en dataframes pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fulltext</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U . S .- JAPA...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CHINA DAILY SAYS VERMIN EAT 7 - 12 PCT GRAIN S...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>JAPAN TO REVISE LONG - TERM ENERGY DEMAND DOWN...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER Tha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY Indone...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            fulltext  category\n",
              "0  ASIAN EXPORTERS FEAR DAMAGE FROM U . S .- JAPA...         0\n",
              "1  CHINA DAILY SAYS VERMIN EAT 7 - 12 PCT GRAIN S...         1\n",
              "2  JAPAN TO REVISE LONG - TERM ENERGY DEMAND DOWN...         0\n",
              "3  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER Tha...         1\n",
              "4  INDONESIA SEES CPO PRICE RISING SHARPLY Indone...         0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# On récupère tous les ids des fichiers qui nous intéressent et on les sépare en deux catégories; train et test\n",
        "train_fids = [fid for fid in reuters.fileids() if fid[:5] == 'train']\n",
        "test_fids = [fid for fid in reuters.fileids() if fid[:4] == 'test']\n",
        "\n",
        "# Fonction permettant d'obtenir le texte complet ainsi que la catégorie (1 si grain) du fichier avec l'id spécifié\n",
        "def get_text_and_category(fileid):\n",
        "    fulltext = ' '.join(reuters.words(fileid))\n",
        "    category = 1 if 'grain' in reuters.categories(fileid) else 0\n",
        "    return fulltext, category\n",
        "\n",
        "# Regroupement des données\n",
        "train_data = [(get_text_and_category(fileid)) for fileid in train_fids]\n",
        "train_df = pd.DataFrame(train_data, columns=['fulltext', 'category'])\n",
        "\n",
        "# Création des dataframes pandas\n",
        "test_data = [(get_text_and_category(fileid)) for fileid in test_fids]\n",
        "test_df = pd.DataFrame(test_data, columns=['fulltext', 'category'])\n",
        "\n",
        "train_df.head()\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing grid search...\n",
            "Hyperparameters to be evaluated:\n",
            "{'vect__max_df': (0.6, 0.8), 'vect__min_df': (5, 6), 'vect__norm': ('l1', 'l2')}\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "Done in 15.318s\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from time import time\n",
        "import numpy as np\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    [\n",
        "        (\"vect\", TfidfVectorizer()),\n",
        "        (\"clf\", SGDClassifier(loss=\"hinge\")),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Grille de recherche des paramètres à tester\n",
        "# Nous avons diminué les paramètres à tester pour que ça prenne moins de temps\n",
        "parameter_grid = {\n",
        "    \"vect__max_df\": (0.6, 0.8),\n",
        "    \"vect__min_df\": (5,6),\n",
        "    \"vect__norm\": (\"l1\", \"l2\"),\n",
        "}\n",
        "\n",
        "random_search = GridSearchCV(\n",
        "    scoring='f1',\n",
        "    estimator=pipeline,\n",
        "    param_grid=parameter_grid,\n",
        "    n_jobs=2,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "print(\"Performing grid search...\")\n",
        "print(\"Hyperparameters to be evaluated:\")\n",
        "pprint(parameter_grid)\n",
        "\n",
        "# Recherche des meilleurs hyper-paramètres\n",
        "t0 = time()\n",
        "random_search.fit(train_df['fulltext'], train_df['category'])\n",
        "print(f\"Done in {time() - t0:.3f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters combination found:\n",
            "vect__max_df: 0.8\n",
            "vect__min_df: 5\n",
            "vect__norm: l2\n",
            "F1-score of the best parameters using the inner CV of the random search: 0.935\n",
            "F1-score on test set: 0.906\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Affichage des résultats de la recherche des hyper-paramètres\n",
        "print(\"Best parameters combination found:\")\n",
        "best_parameters = random_search.best_estimator_.get_params()\n",
        "for param_name in sorted(parameter_grid.keys()):\n",
        "    print(f\"{param_name}: {best_parameters[param_name]}\")\n",
        "\n",
        "score = random_search.score(test_df['fulltext'], test_df['category'])\n",
        "print(\n",
        "    \"F1-score of the best parameters using the inner CV of \"\n",
        "    f\"the random search: {random_search.best_score_:.3f}\"\n",
        ")\n",
        "print(f\"F1-score on test set: {score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Autres options et résultats obtenus\n",
        "CountVectorizer n'a pas donné de très bons résultats, nous avons donc gardé TF-IDF.\n",
        "\n",
        "Nous avons comparés ComplementNB et SGDClassifier. SGDClassifier a donné un score environ ~20% plus haut que ComplementNB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f1 : 0.9064748201438849\n",
            "recall : 0.9767441860465116\n",
            "precision : 0.8456375838926175\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "# On récupère le modèle qui a eu le meilleur f1-score\n",
        "best_estimator = random_search.best_estimator_\n",
        "pred = best_estimator.predict(test_df['fulltext'])\n",
        "expected = test_df['category']\n",
        "\n",
        "# Calcul des scores demandés\n",
        "f1 = f1_score(pred, expected)\n",
        "recall = recall_score(pred, expected)\n",
        "precision = precision_score(pred, expected)\n",
        "\n",
        "print('f1 :', f1)\n",
        "print('recall :', recall)\n",
        "print('precision :', precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Résultats finaux\n",
        "Au final, notre meilleur modèle obtient les scores suivants :\n",
        "f1 : 0.906\n",
        "recall : 0.977\n",
        "precision : 0.846"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
