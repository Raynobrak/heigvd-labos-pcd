{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"Logo HEIG-VD\" style=\"width: 80px;\" align=\"right\"/>\n",
    "\n",
    "# Cours PCD - Labo 1\n",
    "\n",
    "# Extraction d'attributs pour la détection du spam\n",
    "\n",
    "## Résumé\n",
    "\n",
    "Le but de ce travail est de réaliser des expériences de classification d'emails en *spam* ou *ham* (c'est-à-dire non-spam) grâce aux données et au code fourni intégralement dans ce notebook Jupyter.\n",
    "\n",
    "Votre objectif sera de chercher les meilleurs attributs (*features*) qu'on puisse extraire des emails, ainsi que les classifieurs, afin de maximiser le score F1 de la classification spam/non-spam.  Les classifieurs utilisés seront de la famille Naive Bayes de Scikit-learn.\n",
    "\n",
    "Veuillez rendre un notebook avec votre meilleure stratégie.  Présentez clairement les options et leurs résultats, et commentez-les brièvement.\n",
    "\n",
    "*Source* : l'idée du labo vient du [premier notebook de Gan Sie Huai](https://github.com/huai99/Email-Spam-Detection-Python) avec plusieurs changements et mises à jour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtention des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez télécharger l'archive d'emails depuis Cyberlearn.  Ou téléchargez les cinq archives commençant par `20030228_` depuis https://spamassassin.apache.org/old/publiccorpus/.  Décompressez-les dans un dossier de votre choix dont vous indiquerez le chemin relatif dans `path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lcsch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import email\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/spam_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_ham_paths = glob.glob(path+'easy_ham/*')\n",
    "easy_ham_2_paths = glob.glob(path+'easy_ham_2/*')\n",
    "hard_ham_paths = glob.glob(path+'hard_ham/*')\n",
    "spam_paths = glob.glob(path+'spam/*')\n",
    "spam_2_paths = glob.glob(path+'spam_2/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_content(email_path):\n",
    "    file = open(email_path,encoding='latin1')\n",
    "    try:\n",
    "        msg = email.message_from_file(file)\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                return part.get_payload() # prints the raw text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "def get_email_content_bulk(email_paths):\n",
    "    email_contents = [get_email_content(o) for o in email_paths]\n",
    "    return email_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Division en *train* et *test*\n",
    "Le code suivant vous permet de diviser les données en *train* et *test* (sous-ensembles d'entraînement et de test).  Veuillez l'exécuter pour créer ces deux ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_path = [easy_ham_paths, easy_ham_2_paths, hard_ham_paths]\n",
    "spam_path = [spam_paths, spam_2_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_sample = np.array([train_test_split(o) for o in ham_path], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875 626\n",
      "1050 351\n",
      "188 63\n"
     ]
    }
   ],
   "source": [
    "for o in [train_test_split(o) for o in ham_path]:\n",
    "    print(len(o[0]), len(o[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_train = np.array([])\n",
    "ham_test = np.array([])\n",
    "for o in ham_sample:\n",
    "    ham_train = np.concatenate((ham_train, o[0]), axis=0)\n",
    "    ham_test  = np.concatenate((ham_test,  o[1]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3113,), (1040,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_train.shape, ham_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_sample = np.array([train_test_split(o) for o in spam_path], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 126\n",
      "1047 350\n"
     ]
    }
   ],
   "source": [
    "for o in [train_test_split(o) for o in spam_path]:\n",
    "    print(len(o[0]), len(o[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_train = np.array([])\n",
    "spam_test = np.array([])\n",
    "for o in spam_sample:\n",
    "    spam_train = np.concatenate((spam_train,o[0]),axis=0)\n",
    "    spam_test = np.concatenate((spam_test,o[1]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1422,), (476,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_train.shape, spam_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_train_label = [0]*ham_train.shape[0]\n",
    "spam_train_label = [1]*spam_train.shape[0]\n",
    "x_train = np.concatenate((ham_train,spam_train))\n",
    "y_train = np.concatenate((ham_train_label,spam_train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_test_label = [0]*ham_test.shape[0]\n",
    "spam_test_label = [1]*spam_test.shape[0]\n",
    "x_test = np.concatenate((ham_test,spam_test))\n",
    "y_test = np.concatenate((ham_test_label,spam_test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shuffle_index = np.random.permutation(np.arange(0,x_train.shape[0]))\n",
    "test_shuffle_index = np.random.permutation(np.arange(0,x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[train_shuffle_index]\n",
    "y_train = y_train[train_shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test[test_shuffle_index]\n",
    "y_test = y_test[test_shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = get_email_content_bulk(x_train)\n",
    "x_test = get_email_content_bulk(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_null(datas,labels):\n",
    "    not_null_idx = [i for i,o in enumerate(datas) if o is not None]\n",
    "    return np.array(datas)[not_null_idx],np.array(labels)[not_null_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = remove_null(x_train,y_train)\n",
    "x_test,y_test = remove_null(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question :** combien d'emails y a-t-il dans le jeu d'entraînement et dans le jeu de test ?  Combien de spam et de ham, respectivement ?  Veuillez écrire le code qui fournit les réponses ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train spam, train ham : 1422 3113\n",
      "test spam, test ham : 476 1040\n",
      "total train, total test : 4535 1516\n",
      "sans compter le fait que les valeurs null sont enlevées par la suite\n"
     ]
    }
   ],
   "source": [
    "print(\"train spam, train ham :\", spam_train.size, ham_train.size)\n",
    "print(\"test spam, test ham :\", spam_test.size, ham_test.size)\n",
    "print(\"total train, total test :\", spam_train.size + ham_train.size, spam_test.size + ham_test.size)\n",
    "print(\"sans compter le fait que les valeurs null sont enlevées par la suite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pré-traitement et nettoyage des données\n",
    "\n",
    "### 3.1 Traitements de bas niveau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlink(word):\n",
    "    return  re.sub(r\"http\\S+\", \"\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(word):\n",
    "    result = word.lower()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_number(word):\n",
    "    result = re.sub(r'\\d+', '', word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(word):\n",
    "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(word):\n",
    "    result = word.strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_newline(word):\n",
    "    return word.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Votre mission :** dans la fonction suivante, insérer dans le tableau `cleaning_utils` les noms des fonctions de pré-traitement que vous souhaitez utiliser afin d'obtenir le meilleur filtre anti-spam.   Veuillez commenter brièvement les raisons de votre choix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_pipeline(sentence):\n",
    "    cleaning_utils = [\n",
    "                        to_lower, # limite le nombre de tokens différents et facilite le comptage des mots\n",
    "                        replace_newline, # on ne traitera pas la mise en page\n",
    "                        remove_whitespace # enlève les espaces en trop\n",
    "                     ]\n",
    "    for o in cleaning_utils:\n",
    "        sentence = o(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [clean_up_pipeline(o) for o in x_train]\n",
    "x_test = [clean_up_pipeline(o) for o in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Traitements linguistiques avec NLTK\n",
    "\n",
    "Veuillez installer la librairie NLTK avec Conda ou `pip`, voir instructions à https://www.nltk.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lcsch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lcsch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(words):\n",
    "    result = [i for i in words if i not in ENGLISH_STOP_WORDS]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_stemmer(words):\n",
    "    return [stemmer.stem(o) for o in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_lemmatizer(words):\n",
    "    return [lemmatizer.lemmatize(o) for o in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Votre mission :** dans la fonction suivante, insérer dans le tableau `cleaning_utils` les noms des fonctions de traitement linguistique que vous souhaitez utiliser afin d'obtenir le meilleur filtre anti-spam.  Veuillez commenter brièvement les raisons de votre choix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token_pipeline(words):\n",
    "    cleaning_utils = [\n",
    "        remove_stop_words, # les stop words n'apportent pas d'informations pertinentes au texte\n",
    "        word_lemmatizer # permet de récupérer la version simplifiée des mots et de garder leur sens dans la phrase (donc plus poussé que le stemmer)\n",
    "    ]\n",
    "    for o in cleaning_utils:\n",
    "        words = o(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque:** la tokenization peut prendre plusieurs minutes, vu la quantité de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [word_tokenize(o) for o in x_train]\n",
    "x_test = [word_tokenize(o) for o in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [clean_token_pipeline(o) for o in x_train]\n",
    "x_test = [clean_token_pipeline(o) for o in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [\" \".join(o) for o in x_train]\n",
    "x_test = [\" \".join(o) for o in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extraction des attributs (*feature extraction*)\n",
    "\n",
    "À cette étape, on transforme les textes des emails en une série d'attributs.  Les méthodes proposées par Scikit-learn permettent de décider quels mots on garde comme attributs, puis les valeurs de chaque attribut pour chaque email.  Résultat : au lieu d'une chaîne de caractères, chaque email est représenté par une suite de nombres (de la même taille pour tous).\n",
    "\n",
    "**Votre mission** : faire des expériences pour choisir entre les deux méthodes suivantes celle qui aboutira aux meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer() \n",
    "# nous avons utilisé le CountVectorizer plutôt que le TfidfVectorizer après comparaison du résultat des deux. Le TfidfVectorizer donnait beaucoup de faux-négatifs (spams détectés comme ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation des textes des emails en vecteurs d'attributs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_feature(vectorizer, raw_tokenize_data):\n",
    "    return vectorizer.transform(raw_tokenize_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_features = convert_to_feature(vectorizer, x_train)\n",
    "x_test_features  = convert_to_feature(vectorizer, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choix de la méthode de classification et entraînement\n",
    "\n",
    "Dans cette section, vous allez pouvoir choisir le classifieur.  On vous propose de travailler avec un modèle de type *Naïve Bayes*, à choisir parmi plusieurs variantes disponibles dans Scikit-learn, et qui sont décrites à https://scikit-learn.org/stable/modules/naive_bayes.html.\n",
    "\n",
    "Voici à titre d'exemple l'utilisation du classifieur *Naïve Bayes Multinomial*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.naive_bayes as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = nb.GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train_features.toarray(), y_train) # entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9981646565285789"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_train_features.toarray(),y_train) # score F1 du modèle sur les données d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Votre mission :** essayer plusieurs classifieurs basés sur l'approche *Naïve Bayes* et choisir celui qui aboutit au meilleur score *sur les données de test*, qui sera calculé dans la section suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation du classifieur entraîné\n",
    "\n",
    "Dans cette section, vous allez demander au classifieur de classer les données de test (non vues à l'entraînement), et calculer (grâce aux réponses correctes) plusieurs scores qui mesurent les performances de détection de spam.  Veuillez répondre aux questions posées en fin de section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(x_test_features.toarray()) # réponses du système sur les données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 97.22%\n",
      "Recall:    83.33%\n",
      "F1-score:  89.74%\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_predict)))\n",
    "print(\"Recall:    {:.2f}%\".format(100 * recall_score(y_test, y_predict)))\n",
    "print(\"F1-score:  {:.2f}%\".format(100 * f1_score(y_test, y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1004,    6],\n",
       "       [  42,  210]], dtype=int64)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_predict) # affichage élémentaire de la matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# affichage graphique de la matrice de confusion\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_predict, display_labels = ['ham', 'spam']) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Veuillez répondre ici** aux questions suivantes :\n",
    "\n",
    "1. Quel était votre premier score F1 obtenu ?  Quel est votre meilleur score ?  De combien avez-vous progressé grâce à vos choix judicieux ?\n",
    "\n",
    "Notre premier essai a donné un F1-score de ~41%. Après quelques essais, nous sommes arrivés à un F1-score de ~94%.\n",
    "\n",
    "2. Quels choix de pré-traitement (section 3) vous ont conduit-e à la meilleure solution ?\n",
    "\n",
    "Nous avons utilisé les traitements de bas-niveau suivants :\n",
    "- to_lower()\n",
    "- remove_whitespace()\n",
    "- replace_newline()\n",
    "Nous avons laissé le reste.\n",
    "\n",
    "Concernant le traitement linguistique, nous avons eu de meilleurs résultats avec le lemmatizer plutôt que le stemmer.\n",
    "\n",
    "3. Quelle représentation des emails vous a conduit-e à la meilleure solution ?\n",
    "\n",
    "Le CountVectorizer nous a, à notre surprise, donné de meilleurs résultats que le TfidfVectorizer. Nous avons été étonnés par ce résultat car le TfidfVectorizer effectue pourtant une analyse plus poussée.\n",
    "\n",
    "4. Quel classifieur vous a conduit-e à la meilleure solution ?\n",
    "\n",
    "Ci-dessous, la liste des classifieurs que nous avons testé avec le F1-score obtenu correspondant :\n",
    "- MultinomialNB -> 94%\n",
    "- BernoulliNB -> 53%\n",
    "- GaussianNB -> 89%\n",
    "- ComplementNB -> 94%\n",
    "\n",
    "Le modèle Multinomial et Complement ont donné les meilleurs résultats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fin du Labo 1.**  Veuillez nettoyer ce notebook en gardant seulement les résultats désirés et vos réponses, l'enregistrer, et le soumettre comme devoir sur Cyberlearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
